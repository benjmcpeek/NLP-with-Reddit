{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f372be8-3092-436e-93e8-912a015d74a9",
   "metadata": {},
   "source": [
    "# Testing Model\n",
    "### Logistic Regression\n",
    "- We will use a Logistic Regression model to determine our best features. We set up two different iterations of our features that progressively filter more. We will run two different Logistic Reg models and see which performs the best under which iteration. Based on which model performs best will determine which iteration we will use for further complex models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4fe4d4de-c0b6-4da3-a16f-331058ffa7ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6aa305c9-e0c8-48b6-bf32-d18eecdf9d84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1980, 9)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed = pd.read_csv('../data/preprocessed.csv')\n",
    "processed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea69173-5131-47df-a9a9-e04fad10a7f8",
   "metadata": {},
   "source": [
    "- We will need to redefine the custom stop words that we explored in preprocessing to use as an iteration option. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4593102f-70da-4bcd-ad57-ddad8001a72b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_stop = stopwords.words('english')\n",
    "new_words = ['ve', 'don', 'amp', 'x200b', 'just', 'didn', 'house', 'time', 'said', 'isn']\n",
    "custom_stop.extend(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eccb6259-6f9d-493e-8a08-0409b9fd74af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title               0\n",
       "text                2\n",
       "subreddit           0\n",
       "name                0\n",
       "text_length         0\n",
       "text_word_length    0\n",
       "title_text          1\n",
       "reg_text            1\n",
       "spacy_text          1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8bbbd207-fd6a-4018-b4c6-2c10e71f1b68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1977 entries, 0 to 1979\n",
      "Data columns (total 9 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   title             1977 non-null   object \n",
      " 1   text              1977 non-null   object \n",
      " 2   subreddit         1977 non-null   float64\n",
      " 3   name              1977 non-null   object \n",
      " 4   text_length       1977 non-null   int64  \n",
      " 5   text_word_length  1977 non-null   int64  \n",
      " 6   title_text        1977 non-null   object \n",
      " 7   reg_text          1977 non-null   object \n",
      " 8   spacy_text        1977 non-null   object \n",
      "dtypes: float64(1), int64(2), object(6)\n",
      "memory usage: 154.5+ KB\n"
     ]
    }
   ],
   "source": [
    "processed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b769a54-7154-4128-9525-21952b3e828f",
   "metadata": {},
   "source": [
    "- Because we are seeing only three rows with missing values out of 1,980 we will drop those three rows.\n",
    "- Three rows in our dataset represents 0.15% of our data which is minimal enough to justify dropping them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33616563-5c37-4b95-a957-cd860c8e75c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1980, 9)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "355bb09c-1ccf-4576-aa19-c311c07e18f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processed.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "652e836b-8b41-473f-aac6-56b68170f415",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1977, 9)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f19b0a7c-60db-467f-afc5-4409eddc0ee0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>name</th>\n",
       "      <th>text_length</th>\n",
       "      <th>text_word_length</th>\n",
       "      <th>title_text</th>\n",
       "      <th>reg_text</th>\n",
       "      <th>spacy_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My Mother's Russian Neighbour</td>\n",
       "      <td>So this story isn't mine but my mother's from ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>t3_15ga20a</td>\n",
       "      <td>647</td>\n",
       "      <td>117</td>\n",
       "      <td>My Mother's Russian NeighbourSo this story isn...</td>\n",
       "      <td>my mother s russian neighbourso this story isn...</td>\n",
       "      <td>Mother Russian NeighbourSo story mine mother e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Im a weird virgin...</td>\n",
       "      <td>Ok so I (17f) am still a virgin. I never had a...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>t3_15fsclg</td>\n",
       "      <td>935</td>\n",
       "      <td>195</td>\n",
       "      <td>Im a weird virgin...Ok so I (17f) am still a v...</td>\n",
       "      <td>im a weird virgin ok so i 17f am still a virgi...</td>\n",
       "      <td>weird virgin virgin have real boyfriend more t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FUNNY STORY</td>\n",
       "      <td>I WENT TO A FANCY RESTAURANT AFTER CHURCH WITH...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>t3_15er04c</td>\n",
       "      <td>193</td>\n",
       "      <td>37</td>\n",
       "      <td>FUNNY STORYI WENT TO A FANCY RESTAURANT AFTER ...</td>\n",
       "      <td>funny storyi went to a fancy restaurant after ...</td>\n",
       "      <td>STORYI go fancy RESTAURANT church MY CHURCH CL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arcade poo</td>\n",
       "      <td>As crazy as this sounds, this is a real story!...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>t3_15c4bg1</td>\n",
       "      <td>1600</td>\n",
       "      <td>334</td>\n",
       "      <td>Arcade pooAs crazy as this sounds, this is a r...</td>\n",
       "      <td>arcade pooas crazy as this sounds this is a re...</td>\n",
       "      <td>Arcade pooa crazy sound real story go arcade f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>My grandpas filter (or lack of)</td>\n",
       "      <td>I (16m) live with my grandparents for most of ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>t3_15c53hk</td>\n",
       "      <td>1650</td>\n",
       "      <td>314</td>\n",
       "      <td>My grandpas filter (or lack of)I (16m) live wi...</td>\n",
       "      <td>my grandpas filter or lack of i 16m live with ...</td>\n",
       "      <td>grandpas filter lack live grandparent most sum...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             title  \\\n",
       "0    My Mother's Russian Neighbour   \n",
       "1             Im a weird virgin...   \n",
       "3                      FUNNY STORY   \n",
       "4                       Arcade poo   \n",
       "5  My grandpas filter (or lack of)   \n",
       "\n",
       "                                                text  subreddit        name  \\\n",
       "0  So this story isn't mine but my mother's from ...        0.0  t3_15ga20a   \n",
       "1  Ok so I (17f) am still a virgin. I never had a...        0.0  t3_15fsclg   \n",
       "3  I WENT TO A FANCY RESTAURANT AFTER CHURCH WITH...        0.0  t3_15er04c   \n",
       "4  As crazy as this sounds, this is a real story!...        0.0  t3_15c4bg1   \n",
       "5  I (16m) live with my grandparents for most of ...        0.0  t3_15c53hk   \n",
       "\n",
       "   text_length  text_word_length  \\\n",
       "0          647               117   \n",
       "1          935               195   \n",
       "3          193                37   \n",
       "4         1600               334   \n",
       "5         1650               314   \n",
       "\n",
       "                                          title_text  \\\n",
       "0  My Mother's Russian NeighbourSo this story isn...   \n",
       "1  Im a weird virgin...Ok so I (17f) am still a v...   \n",
       "3  FUNNY STORYI WENT TO A FANCY RESTAURANT AFTER ...   \n",
       "4  Arcade pooAs crazy as this sounds, this is a r...   \n",
       "5  My grandpas filter (or lack of)I (16m) live wi...   \n",
       "\n",
       "                                            reg_text  \\\n",
       "0  my mother s russian neighbourso this story isn...   \n",
       "1  im a weird virgin ok so i 17f am still a virgi...   \n",
       "3  funny storyi went to a fancy restaurant after ...   \n",
       "4  arcade pooas crazy as this sounds this is a re...   \n",
       "5  my grandpas filter or lack of i 16m live with ...   \n",
       "\n",
       "                                          spacy_text  \n",
       "0  Mother Russian NeighbourSo story mine mother e...  \n",
       "1  weird virgin virgin have real boyfriend more t...  \n",
       "3  STORYI go fancy RESTAURANT church MY CHURCH CL...  \n",
       "4  Arcade pooa crazy sound real story go arcade f...  \n",
       "5  grandpas filter lack live grandparent most sum...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1057e796-9567-4e8e-a7b4-009edcd02b89",
   "metadata": {},
   "source": [
    "## Creating two different Logistic Reg Models \n",
    "- Our X_1 will be a model using features from our RegExp iteration.\n",
    "- Our X_2 will be a model using features from our Spacy iteration. \n",
    "- Because our baseline accuracy is a low 52.6% we want to see our Logistic Reg models score as well or better than our baseline. \n",
    "- I will use the performance of my best Logistic Regression model to determine how successful my other models will perform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91b38c23-2729-41d5-af01-66fee2810676",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_1 = processed['reg_text']\n",
    "y_1 = processed['subreddit']\n",
    "\n",
    "X_2 = processed['spacy_text']\n",
    "y_2 = processed['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f5583bd-ae26-44b6-8375-bafa387c961e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit\n",
       "1.0    0.52605\n",
       "0.0    0.47395\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Null Baseline \n",
    "y_2.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e92e747f-b146-415e-b3da-2f280c406abf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_1, y_1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84176cbe-b81e-48d1-9251-eb001c34d389",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_2, y_2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcc48fb-5e0d-4060-8635-0cfbf0be2187",
   "metadata": {},
   "source": [
    "- I am only choosing two hyperparameters because I am mainly interested in the impact of my two iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af54bb1c-9142-49e8-920e-af7406b536bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('tri', TfidfVectorizer()),\n",
    "    ('lr', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20741bee-6633-44c4-8017-9393b19cb5d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe_params = {\n",
    "    'tri__stop_words': [None, 'english', custom_stop],\n",
    "    'tri__ngram_range': [(1, 1), (2, 2)]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d7d841-3b06-4823-9864-e26386645723",
   "metadata": {},
   "source": [
    "### RegExp Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6251bf4-0d28-498a-b3f6-5ab61db7cdd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe_gs = GridSearchCV(pipe, param_grid=pipe_params, cv = 5, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc8c8aab-028a-48a0-af05-56908e6482f3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "[CV] END ......tri__ngram_range=(1, 1), tri__stop_words=None; total time=   0.2s\n",
      "[CV] END ......tri__ngram_range=(1, 1), tri__stop_words=None; total time=   0.2s\n",
      "[CV] END ......tri__ngram_range=(1, 1), tri__stop_words=None; total time=   0.3s\n",
      "[CV] END ......tri__ngram_range=(1, 1), tri__stop_words=None; total time=   0.2s\n",
      "[CV] END ......tri__ngram_range=(1, 1), tri__stop_words=None; total time=   0.2s\n",
      "[CV] END ...tri__ngram_range=(1, 1), tri__stop_words=english; total time=   0.2s\n",
      "[CV] END ...tri__ngram_range=(1, 1), tri__stop_words=english; total time=   0.2s\n",
      "[CV] END ...tri__ngram_range=(1, 1), tri__stop_words=english; total time=   0.2s\n",
      "[CV] END ...tri__ngram_range=(1, 1), tri__stop_words=english; total time=   0.3s\n",
      "[CV] END ...tri__ngram_range=(1, 1), tri__stop_words=english; total time=   0.2s\n",
      "[CV] END tri__ngram_range=(1, 1), tri__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 've', 'don', 'amp', 'x200b', 'just', 'didn', 'house', 'time', 'said', 'isn']; total time=   0.3s\n",
      "[CV] END tri__ngram_range=(1, 1), tri__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 've', 'don', 'amp', 'x200b', 'just', 'didn', 'house', 'time', 'said', 'isn']; total time=   0.2s\n",
      "[CV] END tri__ngram_range=(1, 1), tri__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 've', 'don', 'amp', 'x200b', 'just', 'didn', 'house', 'time', 'said', 'isn']; total time=   0.3s\n",
      "[CV] END tri__ngram_range=(1, 1), tri__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 've', 'don', 'amp', 'x200b', 'just', 'didn', 'house', 'time', 'said', 'isn']; total time=   0.2s\n",
      "[CV] END tri__ngram_range=(1, 1), tri__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 've', 'don', 'amp', 'x200b', 'just', 'didn', 'house', 'time', 'said', 'isn']; total time=   0.3s\n",
      "[CV] END ......tri__ngram_range=(2, 2), tri__stop_words=None; total time=   0.8s\n",
      "[CV] END ......tri__ngram_range=(2, 2), tri__stop_words=None; total time=   0.7s\n",
      "[CV] END ......tri__ngram_range=(2, 2), tri__stop_words=None; total time=   0.9s\n",
      "[CV] END ......tri__ngram_range=(2, 2), tri__stop_words=None; total time=   0.7s\n",
      "[CV] END ......tri__ngram_range=(2, 2), tri__stop_words=None; total time=   0.7s\n",
      "[CV] END ...tri__ngram_range=(2, 2), tri__stop_words=english; total time=   0.6s\n",
      "[CV] END ...tri__ngram_range=(2, 2), tri__stop_words=english; total time=   0.6s\n",
      "[CV] END ...tri__ngram_range=(2, 2), tri__stop_words=english; total time=   0.7s\n",
      "[CV] END ...tri__ngram_range=(2, 2), tri__stop_words=english; total time=   0.6s\n",
      "[CV] END ...tri__ngram_range=(2, 2), tri__stop_words=english; total time=   0.6s\n",
      "[CV] END tri__ngram_range=(2, 2), tri__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 've', 'don', 'amp', 'x200b', 'just', 'didn', 'house', 'time', 'said', 'isn']; total time=   0.7s\n",
      "[CV] END tri__ngram_range=(2, 2), tri__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 've', 'don', 'amp', 'x200b', 'just', 'didn', 'house', 'time', 'said', 'isn']; total time=   0.6s\n",
      "[CV] END tri__ngram_range=(2, 2), tri__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 've', 'don', 'amp', 'x200b', 'just', 'didn', 'house', 'time', 'said', 'isn']; total time=   0.8s\n",
      "[CV] END tri__ngram_range=(2, 2), tri__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 've', 'don', 'amp', 'x200b', 'just', 'didn', 'house', 'time', 'said', 'isn']; total time=   0.6s\n",
      "[CV] END tri__ngram_range=(2, 2), tri__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 've', 'don', 'amp', 'x200b', 'just', 'didn', 'house', 'time', 'said', 'isn']; total time=   0.7s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;tri&#x27;, TfidfVectorizer()),\n",
       "                                       (&#x27;lr&#x27;, LogisticRegression())]),\n",
       "             param_grid={&#x27;tri__ngram_range&#x27;: [(1, 1), (2, 2)],\n",
       "                         &#x27;tri__stop_words&#x27;: [None, &#x27;english&#x27;,\n",
       "                                             [&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;,\n",
       "                                              &#x27;our&#x27;, &#x27;ours&#x27;, &#x27;ourselves&#x27;, &#x27;you&#x27;,\n",
       "                                              &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;, &quot;you&#x27;ll&quot;,\n",
       "                                              &quot;you&#x27;d&quot;, &#x27;your&#x27;, &#x27;yours&#x27;,\n",
       "                                              &#x27;yourself&#x27;, &#x27;yourselves&#x27;, &#x27;he&#x27;,\n",
       "                                              &#x27;him&#x27;, &#x27;his&#x27;, &#x27;himself&#x27;, &#x27;she&#x27;,\n",
       "                                              &quot;she&#x27;s&quot;, &#x27;her&#x27;, &#x27;hers&#x27;, &#x27;herself&#x27;,\n",
       "                                              &#x27;it&#x27;, &quot;it&#x27;s&quot;, &#x27;its&#x27;, &#x27;itself&#x27;, ...]]},\n",
       "             verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;tri&#x27;, TfidfVectorizer()),\n",
       "                                       (&#x27;lr&#x27;, LogisticRegression())]),\n",
       "             param_grid={&#x27;tri__ngram_range&#x27;: [(1, 1), (2, 2)],\n",
       "                         &#x27;tri__stop_words&#x27;: [None, &#x27;english&#x27;,\n",
       "                                             [&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;,\n",
       "                                              &#x27;our&#x27;, &#x27;ours&#x27;, &#x27;ourselves&#x27;, &#x27;you&#x27;,\n",
       "                                              &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;, &quot;you&#x27;ll&quot;,\n",
       "                                              &quot;you&#x27;d&quot;, &#x27;your&#x27;, &#x27;yours&#x27;,\n",
       "                                              &#x27;yourself&#x27;, &#x27;yourselves&#x27;, &#x27;he&#x27;,\n",
       "                                              &#x27;him&#x27;, &#x27;his&#x27;, &#x27;himself&#x27;, &#x27;she&#x27;,\n",
       "                                              &quot;she&#x27;s&quot;, &#x27;her&#x27;, &#x27;hers&#x27;, &#x27;herself&#x27;,\n",
       "                                              &#x27;it&#x27;, &quot;it&#x27;s&quot;, &#x27;its&#x27;, &#x27;itself&#x27;, ...]]},\n",
       "             verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tri&#x27;, TfidfVectorizer()), (&#x27;lr&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tri', TfidfVectorizer()),\n",
       "                                       ('lr', LogisticRegression())]),\n",
       "             param_grid={'tri__ngram_range': [(1, 1), (2, 2)],\n",
       "                         'tri__stop_words': [None, 'english',\n",
       "                                             ['i', 'me', 'my', 'myself', 'we',\n",
       "                                              'our', 'ours', 'ourselves', 'you',\n",
       "                                              \"you're\", \"you've\", \"you'll\",\n",
       "                                              \"you'd\", 'your', 'yours',\n",
       "                                              'yourself', 'yourselves', 'he',\n",
       "                                              'him', 'his', 'himself', 'she',\n",
       "                                              \"she's\", 'her', 'hers', 'herself',\n",
       "                                              'it', \"it's\", 'its', 'itself', ...]]},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f77f22a8-2084-4784-a28d-a0071f3633d7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tri__ngram_range': (1, 1),\n",
       " 'tri__stop_words': ['i',\n",
       "  'me',\n",
       "  'my',\n",
       "  'myself',\n",
       "  'we',\n",
       "  'our',\n",
       "  'ours',\n",
       "  'ourselves',\n",
       "  'you',\n",
       "  \"you're\",\n",
       "  \"you've\",\n",
       "  \"you'll\",\n",
       "  \"you'd\",\n",
       "  'your',\n",
       "  'yours',\n",
       "  'yourself',\n",
       "  'yourselves',\n",
       "  'he',\n",
       "  'him',\n",
       "  'his',\n",
       "  'himself',\n",
       "  'she',\n",
       "  \"she's\",\n",
       "  'her',\n",
       "  'hers',\n",
       "  'herself',\n",
       "  'it',\n",
       "  \"it's\",\n",
       "  'its',\n",
       "  'itself',\n",
       "  'they',\n",
       "  'them',\n",
       "  'their',\n",
       "  'theirs',\n",
       "  'themselves',\n",
       "  'what',\n",
       "  'which',\n",
       "  'who',\n",
       "  'whom',\n",
       "  'this',\n",
       "  'that',\n",
       "  \"that'll\",\n",
       "  'these',\n",
       "  'those',\n",
       "  'am',\n",
       "  'is',\n",
       "  'are',\n",
       "  'was',\n",
       "  'were',\n",
       "  'be',\n",
       "  'been',\n",
       "  'being',\n",
       "  'have',\n",
       "  'has',\n",
       "  'had',\n",
       "  'having',\n",
       "  'do',\n",
       "  'does',\n",
       "  'did',\n",
       "  'doing',\n",
       "  'a',\n",
       "  'an',\n",
       "  'the',\n",
       "  'and',\n",
       "  'but',\n",
       "  'if',\n",
       "  'or',\n",
       "  'because',\n",
       "  'as',\n",
       "  'until',\n",
       "  'while',\n",
       "  'of',\n",
       "  'at',\n",
       "  'by',\n",
       "  'for',\n",
       "  'with',\n",
       "  'about',\n",
       "  'against',\n",
       "  'between',\n",
       "  'into',\n",
       "  'through',\n",
       "  'during',\n",
       "  'before',\n",
       "  'after',\n",
       "  'above',\n",
       "  'below',\n",
       "  'to',\n",
       "  'from',\n",
       "  'up',\n",
       "  'down',\n",
       "  'in',\n",
       "  'out',\n",
       "  'on',\n",
       "  'off',\n",
       "  'over',\n",
       "  'under',\n",
       "  'again',\n",
       "  'further',\n",
       "  'then',\n",
       "  'once',\n",
       "  'here',\n",
       "  'there',\n",
       "  'when',\n",
       "  'where',\n",
       "  'why',\n",
       "  'how',\n",
       "  'all',\n",
       "  'any',\n",
       "  'both',\n",
       "  'each',\n",
       "  'few',\n",
       "  'more',\n",
       "  'most',\n",
       "  'other',\n",
       "  'some',\n",
       "  'such',\n",
       "  'no',\n",
       "  'nor',\n",
       "  'not',\n",
       "  'only',\n",
       "  'own',\n",
       "  'same',\n",
       "  'so',\n",
       "  'than',\n",
       "  'too',\n",
       "  'very',\n",
       "  's',\n",
       "  't',\n",
       "  'can',\n",
       "  'will',\n",
       "  'just',\n",
       "  'don',\n",
       "  \"don't\",\n",
       "  'should',\n",
       "  \"should've\",\n",
       "  'now',\n",
       "  'd',\n",
       "  'll',\n",
       "  'm',\n",
       "  'o',\n",
       "  're',\n",
       "  've',\n",
       "  'y',\n",
       "  'ain',\n",
       "  'aren',\n",
       "  \"aren't\",\n",
       "  'couldn',\n",
       "  \"couldn't\",\n",
       "  'didn',\n",
       "  \"didn't\",\n",
       "  'doesn',\n",
       "  \"doesn't\",\n",
       "  'hadn',\n",
       "  \"hadn't\",\n",
       "  'hasn',\n",
       "  \"hasn't\",\n",
       "  'haven',\n",
       "  \"haven't\",\n",
       "  'isn',\n",
       "  \"isn't\",\n",
       "  'ma',\n",
       "  'mightn',\n",
       "  \"mightn't\",\n",
       "  'mustn',\n",
       "  \"mustn't\",\n",
       "  'needn',\n",
       "  \"needn't\",\n",
       "  'shan',\n",
       "  \"shan't\",\n",
       "  'shouldn',\n",
       "  \"shouldn't\",\n",
       "  'wasn',\n",
       "  \"wasn't\",\n",
       "  'weren',\n",
       "  \"weren't\",\n",
       "  'won',\n",
       "  \"won't\",\n",
       "  'wouldn',\n",
       "  \"wouldn't\",\n",
       "  've',\n",
       "  'don',\n",
       "  'amp',\n",
       "  'x200b',\n",
       "  'just',\n",
       "  'didn',\n",
       "  'house',\n",
       "  'time',\n",
       "  'said',\n",
       "  'isn']}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6c11f4f-0410-46f5-a130-b23a75eb3d99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9197151697151696"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check cross val score \n",
    "pipe_gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3dc1616-1ecd-4406-9af8-73dce28b20c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.975033738191633, 0.9232323232323232)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_gs.score(X_train, y_train), pipe_gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bc67c2-35c8-45db-9dae-38980dce7988",
   "metadata": {},
   "source": [
    "- Although the accuracy for our RegExp model is performing pretty high, we can see this model is overfit due to the Test set performing 5% less than the training set. \n",
    "- For our RegExp model we can see that using our custom stop words was the best option however in our Spacy model, (which scored better), using no stop words was the best. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec00769-d531-4c23-9d6c-df94b5017e36",
   "metadata": {},
   "source": [
    "### Spacy Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "166a773d-34f8-4791-a625-981d33090483",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe_gs_2 = GridSearchCV(pipe, param_grid=pipe_params, cv = 5, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b800a85c-60f8-45e2-802e-ba898c0b84ff",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "[CV] END ......tri__ngram_range=(1, 1), tri__stop_words=None; total time=   0.1s\n",
      "[CV] END ......tri__ngram_range=(1, 1), tri__stop_words=None; total time=   0.2s\n",
      "[CV] END ......tri__ngram_range=(1, 1), tri__stop_words=None; total time=   0.1s\n",
      "[CV] END ......tri__ngram_range=(1, 1), tri__stop_words=None; total time=   0.1s\n",
      "[CV] END ......tri__ngram_range=(1, 1), tri__stop_words=None; total time=   0.3s\n",
      "[CV] END ...tri__ngram_range=(1, 1), tri__stop_words=english; total time=   0.1s\n",
      "[CV] END ...tri__ngram_range=(1, 1), tri__stop_words=english; total time=   0.1s\n",
      "[CV] END ...tri__ngram_range=(1, 1), tri__stop_words=english; total time=   0.1s\n",
      "[CV] END ...tri__ngram_range=(1, 1), tri__stop_words=english; total time=   0.2s\n",
      "[CV] END ...tri__ngram_range=(1, 1), tri__stop_words=english; total time=   0.1s\n",
      "[CV] END tri__ngram_range=(1, 1), tri__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 've', 'don', 'amp', 'x200b', 'just', 'didn', 'house', 'time', 'said', 'isn']; total time=   0.2s\n",
      "[CV] END tri__ngram_range=(1, 1), tri__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 've', 'don', 'amp', 'x200b', 'just', 'didn', 'house', 'time', 'said', 'isn']; total time=   0.2s\n",
      "[CV] END tri__ngram_range=(1, 1), tri__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 've', 'don', 'amp', 'x200b', 'just', 'didn', 'house', 'time', 'said', 'isn']; total time=   0.2s\n",
      "[CV] END tri__ngram_range=(1, 1), tri__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 've', 'don', 'amp', 'x200b', 'just', 'didn', 'house', 'time', 'said', 'isn']; total time=   0.1s\n",
      "[CV] END tri__ngram_range=(1, 1), tri__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 've', 'don', 'amp', 'x200b', 'just', 'didn', 'house', 'time', 'said', 'isn']; total time=   0.2s\n",
      "[CV] END ......tri__ngram_range=(2, 2), tri__stop_words=None; total time=   0.5s\n",
      "[CV] END ......tri__ngram_range=(2, 2), tri__stop_words=None; total time=   0.5s\n",
      "[CV] END ......tri__ngram_range=(2, 2), tri__stop_words=None; total time=   0.5s\n",
      "[CV] END ......tri__ngram_range=(2, 2), tri__stop_words=None; total time=   0.4s\n",
      "[CV] END ......tri__ngram_range=(2, 2), tri__stop_words=None; total time=   0.5s\n",
      "[CV] END ...tri__ngram_range=(2, 2), tri__stop_words=english; total time=   0.5s\n",
      "[CV] END ...tri__ngram_range=(2, 2), tri__stop_words=english; total time=   0.4s\n",
      "[CV] END ...tri__ngram_range=(2, 2), tri__stop_words=english; total time=   0.4s\n",
      "[CV] END ...tri__ngram_range=(2, 2), tri__stop_words=english; total time=   0.4s\n",
      "[CV] END ...tri__ngram_range=(2, 2), tri__stop_words=english; total time=   0.5s\n",
      "[CV] END tri__ngram_range=(2, 2), tri__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 've', 'don', 'amp', 'x200b', 'just', 'didn', 'house', 'time', 'said', 'isn']; total time=   0.5s\n",
      "[CV] END tri__ngram_range=(2, 2), tri__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 've', 'don', 'amp', 'x200b', 'just', 'didn', 'house', 'time', 'said', 'isn']; total time=   0.5s\n",
      "[CV] END tri__ngram_range=(2, 2), tri__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 've', 'don', 'amp', 'x200b', 'just', 'didn', 'house', 'time', 'said', 'isn']; total time=   0.5s\n",
      "[CV] END tri__ngram_range=(2, 2), tri__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 've', 'don', 'amp', 'x200b', 'just', 'didn', 'house', 'time', 'said', 'isn']; total time=   0.5s\n",
      "[CV] END tri__ngram_range=(2, 2), tri__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 've', 'don', 'amp', 'x200b', 'just', 'didn', 'house', 'time', 'said', 'isn']; total time=   0.5s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;tri&#x27;, TfidfVectorizer()),\n",
       "                                       (&#x27;lr&#x27;, LogisticRegression())]),\n",
       "             param_grid={&#x27;tri__ngram_range&#x27;: [(1, 1), (2, 2)],\n",
       "                         &#x27;tri__stop_words&#x27;: [None, &#x27;english&#x27;,\n",
       "                                             [&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;,\n",
       "                                              &#x27;our&#x27;, &#x27;ours&#x27;, &#x27;ourselves&#x27;, &#x27;you&#x27;,\n",
       "                                              &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;, &quot;you&#x27;ll&quot;,\n",
       "                                              &quot;you&#x27;d&quot;, &#x27;your&#x27;, &#x27;yours&#x27;,\n",
       "                                              &#x27;yourself&#x27;, &#x27;yourselves&#x27;, &#x27;he&#x27;,\n",
       "                                              &#x27;him&#x27;, &#x27;his&#x27;, &#x27;himself&#x27;, &#x27;she&#x27;,\n",
       "                                              &quot;she&#x27;s&quot;, &#x27;her&#x27;, &#x27;hers&#x27;, &#x27;herself&#x27;,\n",
       "                                              &#x27;it&#x27;, &quot;it&#x27;s&quot;, &#x27;its&#x27;, &#x27;itself&#x27;, ...]]},\n",
       "             verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;tri&#x27;, TfidfVectorizer()),\n",
       "                                       (&#x27;lr&#x27;, LogisticRegression())]),\n",
       "             param_grid={&#x27;tri__ngram_range&#x27;: [(1, 1), (2, 2)],\n",
       "                         &#x27;tri__stop_words&#x27;: [None, &#x27;english&#x27;,\n",
       "                                             [&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;,\n",
       "                                              &#x27;our&#x27;, &#x27;ours&#x27;, &#x27;ourselves&#x27;, &#x27;you&#x27;,\n",
       "                                              &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;, &quot;you&#x27;ll&quot;,\n",
       "                                              &quot;you&#x27;d&quot;, &#x27;your&#x27;, &#x27;yours&#x27;,\n",
       "                                              &#x27;yourself&#x27;, &#x27;yourselves&#x27;, &#x27;he&#x27;,\n",
       "                                              &#x27;him&#x27;, &#x27;his&#x27;, &#x27;himself&#x27;, &#x27;she&#x27;,\n",
       "                                              &quot;she&#x27;s&quot;, &#x27;her&#x27;, &#x27;hers&#x27;, &#x27;herself&#x27;,\n",
       "                                              &#x27;it&#x27;, &quot;it&#x27;s&quot;, &#x27;its&#x27;, &#x27;itself&#x27;, ...]]},\n",
       "             verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tri&#x27;, TfidfVectorizer()), (&#x27;lr&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tri', TfidfVectorizer()),\n",
       "                                       ('lr', LogisticRegression())]),\n",
       "             param_grid={'tri__ngram_range': [(1, 1), (2, 2)],\n",
       "                         'tri__stop_words': [None, 'english',\n",
       "                                             ['i', 'me', 'my', 'myself', 'we',\n",
       "                                              'our', 'ours', 'ourselves', 'you',\n",
       "                                              \"you're\", \"you've\", \"you'll\",\n",
       "                                              \"you'd\", 'your', 'yours',\n",
       "                                              'yourself', 'yourselves', 'he',\n",
       "                                              'him', 'his', 'himself', 'she',\n",
       "                                              \"she's\", 'her', 'hers', 'herself',\n",
       "                                              'it', \"it's\", 'its', 'itself', ...]]},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_gs_2.fit(X_train_2, y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ed8a91e-492c-4c23-8242-07e959df4284",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tri__ngram_range': (1, 1), 'tri__stop_words': None}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_gs_2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "647b257c-fcd1-4f9d-a2b2-77f73aa27e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9217353717353717"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check cross val score \n",
    "pipe_gs_2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4190efc6-60bf-497f-8afb-9d417d1fc923",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9689608636977058, 0.9252525252525252)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_gs_2.score(X_train_2, y_train_2), pipe_gs_2.score(X_test_2, y_test_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22eb9818-2613-4fd7-81fb-c4ac3bb418f5",
   "metadata": {},
   "source": [
    "- It may seem unusual for a model to perform the best by not filtering stop words but for this model Spacy had already removed nearly all stop words making this hyperparameter a bit redundant. \n",
    "- We can see that our Spacy model is performing slightly better than our RegExp model. Checking our cross-val-score, we can see that the Spacy and RegExp models performed fairly similarly. However looking at our Train and Test scores we can see that our Spacy model is less overfit than our RegExp model. \n",
    "- We will use our Spacy Iterations to run two other models to find our best accuracy. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
